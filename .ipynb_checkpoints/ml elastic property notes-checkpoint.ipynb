{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featurize the data (machine readable representation of formula)\n",
    "\n",
    "## Split the data into training set (for building the model) and test set (for ensuring out-of-sample performance)\n",
    "\n",
    "## Consider scaling the features\n",
    "\n",
    "Once we have our features. We will probably want to scale them. \n",
    "\n",
    "Most machine learning algorithms rely on gradient descent to find the optimal model fit. \n",
    "\n",
    "If features have drastically different values, issues can arrise with during the search for our optimum. \n",
    "\n",
    "**Inser image of scaled vs non-scaled for gradient boosting\n",
    "\n",
    "Algorithms that will most likely require gradient descent:\n",
    "- Linear Regression, \n",
    "- Ridge Regression, \n",
    "- Lasso,\n",
    "- Support Vector Machines, \n",
    "- Neural Networks, \n",
    "- Logistic regression, etc.\n",
    "\n",
    "Algorithms that do not require gradient descent (usually decision tree based):\n",
    "- Decision Tree, \n",
    "- Random Forest,\n",
    "- Gradient Tree Boosting, etc.\n",
    "\n",
    "## Select desired algorithms\n",
    "\n",
    "At this point, we can decide on the algorithm we want to use.\n",
    "\n",
    "Here are some general characteristics for a few popular algorithms\n",
    "- random forest (rf): \n",
    "    - usually works\n",
    "    - doesn't need any feature scaling\n",
    "    - little effort to get good results\n",
    "    - scales pretty good with data\n",
    "    - can learn complex non-linear relationships\n",
    "\n",
    "- support vector machine (svm): (regression=(SVR), classification=(SVC))\n",
    "    - requires detailed parameter tuning\n",
    "    - models are slow to train with large amounts of data ( $\\approx 10000$)\n",
    "    - resulting models are generally very good\n",
    "    - can learn complex non-linear relationships\n",
    "    \n",
    "- linear regression:\n",
    "    - a simple approach, has some interpretability\n",
    "    - regularization can be added if overfitting\n",
    "        - L1 regularization = Lasso\n",
    "        - L2 regularization = Ridge Regression\n",
    "    - extermely fast. Works with huge amounts of data\n",
    "    - very limited model complexity (often underfits)\n",
    "\n",
    "- neural network:\n",
    "    - more difficult to implement (reasonably sized models require additional software)\n",
    "    - requires detailed parameter tuning\n",
    "    - scales well with large amounts of data\n",
    "    - networks can be designed to learn arbitrarily complex relationships\n",
    "        - particularly useful for image data\n",
    "    - usually considered the most accurate\n",
    "\n",
    "A safe bet is to try many or all of them! Lets go simple and use a support vector regression (SVR).\n",
    "\n",
    "## Optimize algorithm parameters\n",
    "\n",
    "Regardless of the algorithm we choose, we will want to optimize the model parameters to get the best performance possible. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Check performance on the test set\n",
    "\n",
    "## Make predictions on future compounds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
